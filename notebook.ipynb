{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 861, which is longer than the specified 600\n",
      "Created a chunk of size 1573, which is longer than the specified 600\n",
      "Created a chunk of size 1637, which is longer than the specified 600\n",
      "Created a chunk of size 1581, which is longer than the specified 600\n",
      "Created a chunk of size 1539, which is longer than the specified 600\n",
      "Created a chunk of size 1801, which is longer than the specified 600\n",
      "Created a chunk of size 1686, which is longer than the specified 600\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from typing import Text\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"Commentaires\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/plan_weekly.xlsx\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    doc.page_content = doc.page_content.replace('\\n', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir)\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "    # (\"system\",\n",
    "    #  \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer, just say you don't know. Don't make it up:\\n{context}\"),\n",
    "    # (\"human\", \"{question}\")\n",
    "# ])\n",
    "\n",
    "# chain = {\"context\": retriever, \"question\": RunnablePassthrough(\n",
    "# )} | prompt | llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='목요일에는 출장으로 대전에 갔다가 회의를 진행하고 돌아온 후에 메일 처리와 아이 돌봄을 하였습니다. 또한, 지하 매점에서 와인을 구매하였습니다.')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_doc_prompt = ChatPromptTemplate.format_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim.\n",
    "            -------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs['documents']\n",
    "    question = inputs['question']\n",
    "    return \"\\n\\n\".join(\n",
    "        map_doc_chain.invoke({\"context\":doc.page_content, \"question\":question}).content\n",
    "        for doc in documents\n",
    "    )\n",
    "    \n",
    "\n",
    "\n",
    "map_chain = {\"documents\": retriever, \"question\": RunnablePassthrough()} | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_message([\n",
    "    (\"system\",\n",
    "     \"Given the following extracted parts of a long document and a question, create a final answer. If you don't know the answer, just say you don't know. Don't make it up:\\n{context}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "\n",
    "chain = {\"context\": map_chain, \"question\":RunnablePassthrough()} |final_prompt | llm\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
